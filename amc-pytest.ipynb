{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## abridged metaprogramming classics - this episode: pytest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](ac-lotr.jpg)\n",
    "<br>\n",
    "<center style=\"font-size: 20px; font-family: Source Code Pro Medium; font-weight: bold;\">&lt;&lt;METADATA&gt;&gt; BOOK: ABRIDGED CLASSICS; AUTHOR: JOHN ATKINSON&lt;&lt;/METADATA&gt;&gt;</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "I am afraid i need every minute to make this half hour worth while for you, but I'll be glad to listen to your questions and comments in the coffee break afterwards or whenever in the interwebs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# let's imagine the internet is down 😱"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](internet-is-down.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* we're bored"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* we have a laptop with Python3.8 installed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* we want to learn a bit about metaprogramming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# logical conclusion: re-implement pytest from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## (about 0.6% of it)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# use inbuilt functionality directly and max 3 stdlib imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![](luciano-ramalho-core-features.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "<br>\n",
    "<center style=\"font-size: 20px; font-family: Source Code Pro Medium; font-weight: bold;\">&lt;&lt;METADATA&gt;&gt; TALK: BEYOND PARADIGMS: A NEW KEY TO GROK PYTHON & OTHER LANGUAGES; SPEAKER: LUCIANO RAMALHO&lt;&lt;/METADATA&gt;&gt;</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# minimal theoretical knowledge to follow this talk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## modules and functions are first class citizens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&nbsp;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* assignable / passable / returnable&nbsp;&nbsp;&nbsp; => &nbsp; `lobster = eggs(spam)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* inspectable&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; => &nbsp; `dir(spam)`, `spam.__name__`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "#### everything that looks like `__foo__` is part of internal Python mechanics and does interesting stuff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* mutable&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; => &nbsp; `sys.path = []`, `spam.new_attr = 42`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### good reads from the Python docs: [Execution Model](https://docs.python.org/3/reference/executionmodel.html)  | [Data Model](https://docs.python.org/3/reference/datamodel.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# what is pytest?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&nbsp;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "pytest is a very mature and powerful testing framework with a low entrance barrier. It does its best to make it easy to get started writing and running tests. Due to its feature set and plugin based architecture it is very modifiable and extensible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# https://pytest.org/en/latest/talks.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&nbsp;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "##### or hop in your time machine, travel back to pycon.de 2019 and watch the introduction by @hackebrot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# metawhatchamacallit?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Let's have a quick look at the definition of metaprogramming, so that we can spot, when we are doing it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "> Metaprogramming is a programming technique in which computer programs have the ability to treat other programs as their data. It means that a program can be designed to read, generate, analyze or transform other programs, and even modify itself while running. \n",
    ">\n",
    "> **[lots more]**\n",
    ">\n",
    "> &mdash;&mdash; [Wikipedia: Metaprogramming](https://en.wikipedia.org/wiki/Metaprogramming)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "> Metaprogramming is using code to do stuff with code.\n",
    ">\n",
    "> &mdash;&mdash; Abridged Wikipedia (if it would exist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# first feature:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "## automatic test discovery and execution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## find and run functions with the right name in the right place"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### `**/test_*.py:test_*`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# [pytest] a test suite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "!tree tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "##### a bunch of Python modules containing test functions in arbitrarily nested folders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# [pytest] minimal test anatomy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# %load tests/basics/test_demo.py\n",
    "def test_passes():\n",
    "    \"\"\"function is executed without error => test passes.\"\"\"\n",
    "\n",
    "\n",
    "def test_fails_assert():\n",
    "    assert 0, \"assertion fails => test fails\"\n",
    "\n",
    "\n",
    "def test_fails_exception():\n",
    "    int(\"x\")  # error anywhere => test fails\"\n",
    "\n",
    "\n",
    "def non_test_function():\n",
    "    \"\"\"This is not a test.\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# [pytest] only collect the tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tests/basics/test_demo.py::test_passes\r\n",
      "tests/basics/test_demo.py::test_fails_assert\r\n",
      "tests/basics/test_demo.py::test_fails_exception\r\n",
      "tests/basics/folder/test_in_a_folder.py::test_passes_in_a_folder\r\n",
      "tests/basics/folder/test_in_a_folder.py::test_fails_in_a_folder\r\n",
      "\r\n",
      "\u001B[33m\u001B[1mno tests ran in 0.03s\u001B[0m\r\n"
     ]
    }
   ],
   "source": [
    "!pytest --collect-only tests/basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# [pico_pytest] collect paths to the test modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tests/basics/test_demo.py\n",
      "tests/basics/folder/test_in_a_folder.py\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path  # stdlib import number one 😱\n",
    "\n",
    "def pico_pytest(path, collect_only=False):\n",
    "    \"\"\"Re-implementation of pytest - how hard can it be?\"\"\"\n",
    "    paths = Path(path).glob(\"**/test_*.py\")\n",
    "    if collect_only:\n",
    "        print(\"\\n\".join([str(path) for path in paths]))\n",
    "\n",
    "pico_pytest(\"tests/basics\", collect_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "##### (no metaprogramming in sight yet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# next step:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "## load a test module programmatically"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# [fail] old skool import: hardcoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    },
    "slideshow": {
     "slide_type": ""
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    import tests.collect.test_demo\n",
    "except ImportError as e:\n",
    "    print(f\"{e=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "##### (because it's not in a package (no `__init__.py`))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# [success] old skool import: hardcoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    },
    "slideshow": {
     "slide_type": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/talk', '/usr/local/lib/python38.zip']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys  # stdlib import number two 😱\n",
    "sys.path[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "sys.path.append(\"tests/basics\")\n",
    "import test_demo\n",
    "test_demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# [stdlib] new skool import: programmatically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    },
    "slideshow": {
     "slide_type": ""
    }
   },
   "outputs": [],
   "source": [
    "import importlib  # stdlib import number three 😱\n",
    "help(importlib)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "is_executing": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# let's not use a library - we can use a builtin instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    },
    "slideshow": {
     "slide_type": ""
    }
   },
   "outputs": [],
   "source": [
    "del importlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# new skool import: programmatically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    },
    "slideshow": {
     "slide_type": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__import__(name, globals=None, locals=None, fromlist=(), level=0) -> module\n",
      "\n",
      "Import a module. Because this function is meant for use by the Python\n",
      "interpreter and not for general use, it is better to use\n",
      "importlib.import_module() to programmatically import a module.\n"
     ]
    }
   ],
   "source": [
    "print(__import__.__doc__[:266])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "test_module_from_function = __import__(\"test_demo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "test_demo is test_module_from_function  # modules are singleton objects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "is_executing": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 🍬 `import foo` : syntax sugar for 🍬"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "is_executing": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 🍬 `foo = __import__(\"foo\")` 🍬"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# [pico_pytest] instantiate module objects from paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def pico_pytest(path, collect_only=False):\n",
    "    paths = Path(path).glob(\"**/test_*.py\")\n",
    "    modules = []\n",
    "    for path in paths:\n",
    "        sys.path.append(str(path.parent))\n",
    "        modules.append(__import__(path.stem))\n",
    "    if collect_only:\n",
    "        print(\"\\n\".join([str(module) for module in modules]))\n",
    "\n",
    "pico_pytest(\"tests/basics\", collect_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 🌟 das metaprogrammometer 🌟 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](metaprogrammometer.jpg)\n",
    "<br>\n",
    "<center style=\"font-size: 20px; font-family: Source Code Pro Medium; font-weight: bold;\">&lt;&lt;METADATA&gt;&gt;John Cohen with IBMblr Play Machine&lt;&lt;/METADATA&gt;&gt;</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "* filled in through this input \n",
    "* gently heated in these gals tubs\n",
    "* purified in the pop corn pool\n",
    "* broken up into logical units by this axe\n",
    "* analyzed using the global knowledge about programming\n",
    "* producing an assessment of the metaprogramming techniques and a measurement of the level of metaprogramyness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 🌟 das metaprogrammometer readings 🌟 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "## altering the program itself by loading arbitrary executable code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "## (modules containing test functions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "## and adding them to a data structure for later use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "# 🌟"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# [pico_pytest] pull test module import into a function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def import_module(path):\n",
    "    sys.path.append(str(path.parent))\n",
    "    module = __import__(path.stem)\n",
    "    sys.path.pop()  # can't hurt to tidy up after yourself ...\n",
    "    return module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def pico_pytest(path, collect_only=False):\n",
    "    paths = Path(path).glob(\"**/test_*.py\")\n",
    "    modules = [import_module(path) for path in paths]\n",
    "    if collect_only:\n",
    "        print(\"\\n\".join([str(module) for module in modules]))\n",
    "\n",
    "pico_pytest(\"tests/basics\", collect_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# next step:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "## collecting tests from a module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# the namespace of a module is a `name:object` mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#  it is accessible via `__dict__` (and behaves like one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "test_demo.test_passes              # hardcoded old skool attribute access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "test_demo.__dict__['test_passes']  # access via namespace mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# in case the \".\"-key is broken on your keyboard\n",
    "getattr(test_demo, \"__dict__\")[\"test_passes\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 🍬 access to attributes via dot notation is syntax sugar 🍬"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# [pico_pytest] a function to collect test functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from types import FunctionType  # third stdlib import 😱\n",
    "\n",
    "def collect_test_functions(module):\n",
    "    functions = []\n",
    "    for name, obj in test_demo.__dict__.items():\n",
    "        if (\n",
    "            isinstance(obj, FunctionType) \n",
    "            and name.startswith(\"test_\")\n",
    "        ):\n",
    "            functions.append(obj)\n",
    "    return functions\n",
    "\n",
    "collect_test_functions(test_demo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# let's not use that third import yet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del FunctionType"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# [pico_pytest] a function to collect test functions II"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def collect_test_functions(module):\n",
    "    return [\n",
    "        obj\n",
    "        for obj in module.__dict__.values() if\n",
    "            obj.__class__.__name__ == \"function\" \n",
    "            and obj.__name__.startswith(\"test_\")\n",
    "    ]\n",
    "\n",
    "collect_test_functions(test_demo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# [pico_pytest] put all tests from all modules in a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def pico_pytest(path, collect_only=False):\n",
    "    paths = Path(path).glob(\"**/test_*.py\")\n",
    "    modules = [import_module(path) for path in paths]\n",
    "    tests = []\n",
    "    for module in modules:\n",
    "        tests.extend(collect_test_functions(module))\n",
    "    if collect_only:\n",
    "        print(\"\\n\".join([str(test) for test in tests]))\n",
    "\n",
    "pico_pytest(\"tests/basics\", collect_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# next step:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "## executing the tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# [pico_pytest] a function to execute a test function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def execute_test(f):\n",
    "    result = \".\"\n",
    "    try:\n",
    "        f()\n",
    "        return \"passed\"\n",
    "    except Exception as e:\n",
    "        result = \"F\"\n",
    "        return e\n",
    "    finally:\n",
    "        print(result, end=\"\")\n",
    "        \n",
    "execute_test(test_demo.test_fails_assert)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# [pico_pytest] call all the tests and report on the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def pico_pytest(path, collect_only=False):\n",
    "    paths = Path(path).glob(\"**/test_*.py\")\n",
    "    modules = [import_module(path) for path in paths]\n",
    "    tests = []\n",
    "    for module in modules:\n",
    "        tests.extend(collect_test_functions(module))\n",
    "    if collect_only:\n",
    "        print(\"\\n\".join([str(test) for test in tests]))\n",
    "    else:\n",
    "        results = {test.__name__: execute_test(test) for test in tests}\n",
    "        print(\"\\n\" + \"\\n\".join([f\"{n}: {r!r}\" for n, r in results.items()]))\n",
    "\n",
    "pico_pytest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# [pytest] running `tests/basics`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "!pytest tests/basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# [pico_pytest] running `tests/basics`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    },
    "slideshow": {
     "slide_type": ""
    }
   },
   "outputs": [],
   "source": [
    "pico_pytest(\"tests/basics\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 🌟 das metaprogrammometer readings 🌟 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "## run programmatically collected functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "## from programmatically imported modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "# 🌟"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# second freature:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "## automatic dependency injection (`fixture` decorator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# %load tests/fixtures/test_fixtures.py\n",
    "import pytest\n",
    "\n",
    "\n",
    "@pytest.fixture  # think of this as registering a fixture function\n",
    "def the_answer():\n",
    "    return 42\n",
    "\n",
    "\n",
    "def test_using_fixture_passes(the_answer):  # request fixture result via name\n",
    "    assert the_answer == 42\n",
    "\n",
    "\n",
    "def test_using_fixture_fails(the_answer):\n",
    "    assert the_answer == 23, f\"{the_answer} is not 23 :(\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "&nbsp;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "* test functions can request `fixtures` via their the parameter list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "* `fixtures` are specially marked functions that get executed by pytest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "* their results are injected into the requesting function when executed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "* the connection is made through the name of the function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# next step:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "## collect fixture functions from the test modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# transfer fixture function objects to a map during import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    },
    "slideshow": {
     "slide_type": ""
    }
   },
   "outputs": [],
   "source": [
    "name2fixture = {}  # dict lives as long as the process (test session) lives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def fixture(f):\n",
    "    \"\"\"pico_pytest decorator to register a function as fixture.\"\"\"\n",
    "    name2fixture[f.__name__] = f\n",
    "    print(f\"<fixture({f.__name__}) => registered>\", file=sys.stderr)\n",
    "    \n",
    "fixture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# test transfer directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    },
    "slideshow": {
     "slide_type": ""
    }
   },
   "outputs": [],
   "source": [
    "def syntax_sugar_free():\n",
    "    pass\n",
    "\n",
    "syntax_sugar_free = fixture(syntax_sugar_free)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(syntax_sugar_free)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "name2fixture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 🍬 `@`:  decorator syntax sugar [(PEP 318)](https://www.python.org/dev/peps/pep-0318/) 🍬"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "#### pass a function to a function and assign the result to the name of the original function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "@fixture\n",
    "def with_syntax_sugar():\n",
    "    pass\n",
    "\n",
    "with_syntax_sugar is None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "name2fixture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# test fixture registration from a test module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    },
    "slideshow": {
     "slide_type": ""
    }
   },
   "outputs": [],
   "source": [
    "import pytest\n",
    "\n",
    "pytest.fixture = fixture\n",
    "print(f\"{pytest.fixture.__doc__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 🌟 das metaprogrammometer readings 🌟 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "## replacing library code in a running program"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "## (a.k.a monkeypatching)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "# 🌟 🌟"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# module import triggers fixture registration now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    },
    "slideshow": {
     "slide_type": ""
    }
   },
   "outputs": [],
   "source": [
    "test_fixtures = import_module(Path(\"tests/fixtures/test_fixtures.py\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "name2fixture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# next step:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "## figure out if a test requested a fixture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def test_function(parameter1, parameter2):  # <= we need those parameter names\n",
    "    nameInFunction = 1\n",
    "    \n",
    "dir(test_function)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(dir(test_function.__code__)[-17:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(test_function.__code__.co_argcount, test_function.__code__.co_varnames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# [pico_pytest] add automatic dependency injection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def execute_test(f):\n",
    "    params = f.__code__.co_varnames[:f.__code__.co_argcount]\n",
    "    kwargs = {n: fixture() for n, fixture in name2fixture.items() if n in params}\n",
    "    result = \".\"\n",
    "    try:\n",
    "        f(**kwargs)  # **mapping: call function with key=value keyword arguments\n",
    "        return \"passed\"\n",
    "    except Exception as e:\n",
    "        result = \"F\"\n",
    "        return e\n",
    "    finally:\n",
    "        print(result, end=\"\")\n",
    "\n",
    "pico_pytest(\"tests/fixtures\")  # no change necessary in pico_pytest function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 🌟 das metaprogrammometer readings 🌟 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "## inspecting attributes of callable objects from a mapping,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "## selectively calling them to inject their results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "## into another programmatically called function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "# 🌟 🌟"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# third feature:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "## marking tests with arbitrary names and "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "## selecting subsets of tests via boolean expression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# [pytest] selecting tests using an expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pytest --collect-only -m \"lucy and charlie\" tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# the marked tests we just selected from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# %load tests/marking/test_marking.py\n",
    "import pytest\n",
    "\n",
    "@pytest.mark.charlie                # marker name can be any valid identifier\n",
    "def test_fails_marked_charlie():\n",
    "    assert 0, \"This is bad!\"\n",
    "\n",
    "@pytest.mark.lucy\n",
    "def test_passes_marked_lucy():\n",
    "    pass\n",
    "\n",
    "@pytest.mark.lucy\n",
    "@pytest.mark.charlie                # a test can be marked with several markers\n",
    "def test_passes_marked_lucy_and_charlie():\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# next step:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "# maintain a list of marks on a function object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# problem:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    },
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "## how do we create a function `mark` usable as a decorator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    },
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "## that gets an arbitrary name passed via dot notation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### let's work our way slowly towards this"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# `attach_mark`: a function to maintain a list of marks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    },
    "slideshow": {
     "slide_type": ""
    }
   },
   "outputs": [],
   "source": [
    "def attach_mark(f, m):\n",
    "    try:\n",
    "        f.pico_pytest_marks.append(m)\n",
    "    except AttributeError:\n",
    "        f.pico_pytest_marks = [m]\n",
    "        # ^^^^^^^^^^^^^^^^^ => attach a brand new function attribute\n",
    "    return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def spam():\n",
    "    pass\n",
    "\n",
    "spam = attach_mark(spam, \"patty\")\n",
    "print(spam.pico_pytest_marks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# I can't pass the mark in as a second argument like that"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "# so there has to be another way"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## name lookup in Python: LEGB (local, enclosing, global, builtin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def attach_mark(f):       # we can't pass the name as argument here ...\n",
    "    try:\n",
    "        f.pico_pytest_marks.append(m)   # access m from outside\n",
    "    except AttributeError:\n",
    "        f.pico_pytest_marks = [m]\n",
    "    return f\n",
    "\n",
    "m = \"lucy\"                # we can set it in another accessible scope\n",
    "spam = attach_mark(spam)\n",
    "m = \"franklin\"            # and change as needed\n",
    "spam = attach_mark(spam)\n",
    "spam.pico_pytest_marks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# next step:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "# use a \"portable\" scope by creating a closure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# factory to create decorators carrying their context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    },
    "slideshow": {
     "slide_type": ""
    }
   },
   "outputs": [],
   "source": [
    "def mark_attacher_factory(m):  # function creates a function with enclosed m\n",
    "    def attach_mark(f):\n",
    "        try:\n",
    "            f.pico_pytest_marks.append(m)  # access m from enclosing scope\n",
    "        except AttributeError:\n",
    "            f.pico_pytest_marks = [m]\n",
    "        print(f\"<attach_mark({f.__name__}) => '{m}'>\", file=sys.stderr)\n",
    "        return f\n",
    "    return attach_mark\n",
    "\n",
    "mark_attacher_factory(\"schroeder\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# trying that out directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    },
    "slideshow": {
     "slide_type": ""
    }
   },
   "outputs": [],
   "source": [
    "spam = mark_attacher_factory(\"schroeder\")(spam)\n",
    "print(spam.pico_pytest_marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "@mark_attacher_factory(\"snoopy\")\n",
    "def spam():  # this assigns a new function object to the name \"spam\"\n",
    "    pass\n",
    "\n",
    "print(spam.pico_pytest_marks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# let's visualize this (using pythontutor.com)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## [sugar free](http://pythontutor.com/visualize.html#code=def%20mark_attacher_factory%28m%29%3A%0A%20%20%20%20def%20attach_mark%28f%29%3A%0A%20%20%20%20%20%20%20%20if%20hasattr%28f,%20%22pico_pytest_marks%22%29%3A%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20f.pico_pytest_marks.append%28m%29%0A%20%20%20%20%20%20%20%20else%3A%0A%20%20%20%20%20%20%20%20%20%20%20%20f.pico_pytest_marks%20%3D%20%5Bm%5D%0A%20%20%20%20%20%20%20%20print%28f%22%3Cattach_mark%28%7Bf.__name__%7D%29%20%3D%3E%20'%7Bm%7D'%3E%22%29%0A%20%20%20%20%20%20%20%20return%20f%0A%20%20%20%20return%20attach_mark%0A%0Adef%20spam%28%29%3A%0A%20%20%20%20pass%0A%0Aspam%20%3D%20mark_attacher_factory%28%22schroeder%22%29%28spam%29%0Aprint%28spam.pico_pytest_marks%29&cumulative=true&curInstr=0&heapPrimitives=true&mode=display&origin=opt-frontend.js&py=3&rawInputLstJSON=%5B%5D&textReferences=false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## [with syntax sugar](http://pythontutor.com/visualize.html#code=def%20mark_attacher_factory%28m%29%3A%0A%20%20%20%20def%20attach_mark%28f%29%3A%0A%20%20%20%20%20%20%20%20if%20hasattr%28f,%20%22pico_pytest_marks%22%29%3A%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20f.pico_pytest_marks.append%28m%29%0A%20%20%20%20%20%20%20%20else%3A%0A%20%20%20%20%20%20%20%20%20%20%20%20f.pico_pytest_marks%20%3D%20%5Bm%5D%0A%20%20%20%20%20%20%20%20print%28f%22%3Cattach_mark%28%7Bf.__name__%7D%29%20%3D%3E%20'%7Bm%7D'%3E%22%29%0A%20%20%20%20%20%20%20%20return%20f%0A%20%20%20%20return%20attach_mark%0A%0A%40mark_attacher_factory%28%22snoopy%22%29%0Adef%20spam%28%29%3A%20%20%23%20this%20assigns%20a%20new%20function%20object%20to%20the%20name%20%22spam%22%0A%20%20%20%20pass%0A%0Aprint%28spam.pico_pytest_marks%29&cumulative=true&curInstr=0&heapPrimitives=true&mode=display&origin=opt-frontend.js&py=3&rawInputLstJSON=%5B%5D&textReferences=false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# what does it look like again in pytest?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    },
    "slideshow": {
     "slide_type": ""
    }
   },
   "outputs": [],
   "source": [
    "import pytest\n",
    "\n",
    "@pytest.mark.snoopy  # this is what we want to achieve ...\n",
    "def test_spam():\n",
    "    pass\n",
    "\n",
    "test_spam.pytestmark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# next step:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "## hook into the dot notation of `mark`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# hook into attribute access via `__getattr__`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "class MarkAttacherFactoryFactory:\n",
    "    \"\"\"pico_pytest factory factory to attach marks to tests.\"\"\"\n",
    "    def __getattr__(self, m):  # <- called when normal attribute lookup fails\n",
    "        return mark_attacher_factory(m)\n",
    "\n",
    "mark = MarkAttacherFactoryFactory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "# 🍬 dot notation for all attribute access is syntax sugar 🍬"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# try `mark` with modified attribute access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    },
    "slideshow": {
     "slide_type": ""
    }
   },
   "outputs": [],
   "source": [
    "spam = mark.woodstock(spam)\n",
    "print(spam.pico_pytest_marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "@mark.peggy\n",
    "def spam():\n",
    "    pass\n",
    "\n",
    "print(spam.pico_pytest_marks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# time to test it with a test module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    },
    "slideshow": {
     "slide_type": ""
    }
   },
   "outputs": [],
   "source": [
    "pytest.mark = mark\n",
    "pytest.mark.__doc__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "test_marking = import_module(Path(\"tests/marking/test_marking.py\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# looking at the marked tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    },
    "slideshow": {
     "slide_type": ""
    }
   },
   "outputs": [],
   "source": [
    "tests_marking = collect_test_functions(test_marking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "for test in tests_marking:\n",
    "    print(f\"{test.__name__=:<42} {getattr(test, 'pico_pytest_marks')}\")\n",
    "    #                     ^^^ emoji driven f-string formatting 😄"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 🌟 das metaprogrammometer readings 🌟 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "## closure creation on the fly and "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "## overriding attribute access using a language protocol"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "# 🌟 🌟"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# next step:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "## selecting marked test with a boolean expression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "## using the mark names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# [pytest mark recap] selecting some tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pytest --collect-only -m \"lucy or charlie\" tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# code generation and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    },
    "slideshow": {
     "slide_type": ""
    }
   },
   "outputs": [],
   "source": [
    "OPERATORS = [\"and\", \"or\", \"not\"]\n",
    "\n",
    "def evaluate(expression, names):\n",
    "    tokens = []\n",
    "    for token in expression.split():\n",
    "        if token in names:\n",
    "            tokens.append(\"True\")\n",
    "        elif token in OPERATORS:\n",
    "            tokens.append(token)\n",
    "        else:\n",
    "            tokens.append(\"False\")\n",
    "    return eval(\" \".join(tokens))\n",
    "\n",
    "evaluate(\"spam or eggs\", [\"eggs\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# trying `evaluate` on a few examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    },
    "slideshow": {
     "slide_type": ""
    }
   },
   "outputs": [],
   "source": [
    "for expression, marks, expectation in [                    # <generated code>\n",
    "    (\"lucy or charlie\",      [],                  False),  # \"False or False\"\n",
    "    (\"lucy or charlie\",      [\"lucy\"],            True),   # \"True or False\"\n",
    "    (\"lucy and charlie\",     [\"lucy\"],            False),  # \"True and False\"\n",
    "    (\"lucy and not charlie\", [\"lucy\", \"charlie\"], False),  # \"True and not True\"\n",
    "]:\n",
    "    result = evaluate(expression, marks)\n",
    "    print(f\"{expression:<20} | {str(marks):20} => {result is expectation=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# [pico_pytest] a function to filter the test functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    },
    "slideshow": {
     "slide_type": ""
    }
   },
   "outputs": [],
   "source": [
    "def filter_tests(tests, expression):\n",
    "     return [\n",
    "         test for test in tests \n",
    "         if evaluate(expression, getattr(test, \"pico_pytest_marks\", []))\n",
    "     ]\n",
    "\n",
    "filter_tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 🌟 das metaprogrammometer readings 🌟 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## dynamic code generation and execution "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## in order to decide if programmatically loaded test functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## from programmatically imported modules should be executed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "# 🌟 🌟 🌟 🤯"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# [pico_pytest] add some info about deselected tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    },
    "slideshow": {
     "slide_type": ""
    }
   },
   "outputs": [],
   "source": [
    "def filter_tests(tests, expression):\n",
    "    remaining = [\n",
    "        test for test in tests \n",
    "        if evaluate(expression, getattr(test, \"pico_pytest_marks\", []))\n",
    "    ]\n",
    "    if deselected := len(tests) - len(remaining):     # PEP 572\n",
    "        print(\n",
    "            f\"<filter_tests(...) => {deselected=}>\",  # bpo-36817\n",
    "            file=sys.stderr\n",
    "        )\n",
    "    return remaining\n",
    "\n",
    "filter_tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# try filtering on the test module with marked tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "filter_tests(tests_marking, \"lucy or charlie\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "filter_tests(tests_marking, \"lucy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "filter_tests(tests_marking, \"lucy and not charlie\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# pico_pytest: code complete 😉"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    },
    "slideshow": {
     "slide_type": ""
    }
   },
   "outputs": [],
   "source": [
    "def pico_pytest(path, collect_only=False, mark_expression=None):\n",
    "    paths = Path(path).glob(\"**/test_*.py\")\n",
    "    modules = [import_module(path) for path in paths]\n",
    "    tests = []\n",
    "    for module in modules:\n",
    "        tests.extend(collect_test_functions(module))\n",
    "    if mark_expression:\n",
    "        tests = filter_tests(tests, mark_expression)\n",
    "    if collect_only:\n",
    "        print(\"\\n\".join([str(test) for test in tests]))\n",
    "    else:\n",
    "        results = {test.__name__: execute_test(test) for test in tests}\n",
    "        print(\"\\n\" + \"\\n\".join([f\"{n}: {r!r}\" for n, r in results.items()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# [quick refactoring] one import left ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    },
    "slideshow": {
     "slide_type": ""
    }
   },
   "outputs": [],
   "source": [
    "from itertools import chain  # one import left ...\n",
    "\n",
    "def pico_pytest(path, collect_only=False, mark_expression=None):\n",
    "    paths = Path(path).glob(\"**/test_*.py\")\n",
    "    modules = [import_module(path) for path in paths]\n",
    "    tests = list(chain(*[collect_test_functions(module) for module in modules]))\n",
    "    if mark_expression:\n",
    "        tests = filter_tests(tests, mark_expression)\n",
    "    if collect_only:\n",
    "        print(\"\\n\".join([str(test) for test in tests]))\n",
    "    else:\n",
    "        results = {test.__name__: execute_test(test) for test in tests}\n",
    "        print(\"\\n\" + \"\\n\".join([f\"{n}: {r!r}\" for n, r in results.items()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# acceptance tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# [pico_pytest] collecting tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    },
    "slideshow": {
     "slide_type": ""
    }
   },
   "outputs": [],
   "source": [
    "pico_pytest(\"tests\", collect_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# [pico_pytest] selecting tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    },
    "slideshow": {
     "slide_type": ""
    }
   },
   "outputs": [],
   "source": [
    "pico_pytest(\"tests\", collect_only=True, mark_expression=\"lucy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# [pico_pytest] running tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    },
    "slideshow": {
     "slide_type": ""
    }
   },
   "outputs": [],
   "source": [
    "pico_pytest(\"tests\", mark_expression=\"not lucy and not charlie\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# that's it folks - if you are still here ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](awesome-lotr.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# thank you!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### `<<<=ABRIDGED METAPROGRAMMING CLASSICS - METADATA=>>>`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `[MATERIALS]=> https://gitlab.com/obestwalter/pico-pytest`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `[SPEAKER]=> https://oliver.bestwalter.de`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `<<<=[EMPLOYER]=>>>`\n",
    "![](avira.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### `Avira loves Python and is hiring - come join us!`"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
